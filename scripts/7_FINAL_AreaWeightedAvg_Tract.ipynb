{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bb46d-7592-4b6d-9f48-47091403b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Environment variable for GDAL (optional)\n",
    "os.environ['GDAL_DATA'] = r\"# path to GDAL share directory, if required\"\n",
    "\n",
    "# Configuration\n",
    "input_base = r\"# path to daily weather input files\"\n",
    "output_base = r\"# path to save tract-interpolated outputs\"\n",
    "years_to_process = range(1980, 2020)\n",
    "n_jobs = 12\n",
    "decade = \"\" #Choose right decade vintage \n",
    "batch_size = 25\n",
    "\n",
    "tract_shapefile = r\"# path to 1990 Census Tract shapefile\"\n",
    "tract_id_col = \"GISJOIN2\"\n",
    "\n",
    "group_cols = [\n",
    "    'tmin', 'tmax', 'tmean',\n",
    "    'heat_index_min_c', 'heat_index_max_c', 'heat_index_mean_c',\n",
    "    'wbgt_liljegren_min', 'wbgt_liljegren_max', 'wbgt_liljegren_mean'\n",
    "]\n",
    "\n",
    "# Load a sample file to build static grid\n",
    "sample_file = None\n",
    "for year in years_to_process:\n",
    "    folder = os.path.join(input_base, f\"year={year}\")\n",
    "    if os.path.exists(folder):\n",
    "        for f in os.listdir(folder):\n",
    "            if f.endswith(\".parquet\"):\n",
    "                sample_file = os.path.join(folder, f)\n",
    "                break\n",
    "    if sample_file:\n",
    "        break\n",
    "\n",
    "if not sample_file:\n",
    "    raise FileNotFoundError(\"No .parquet file found to initialize grid.\")\n",
    "\n",
    "df_base = pd.read_parquet(sample_file, columns=['lat', 'lon']).astype({'lat': np.float32, 'lon': np.float32})\n",
    "df_base[\"combo_latlon_id\"] = df_base[\"lat\"].astype(str) + \"_\" + df_base[\"lon\"].astype(str)\n",
    "gdf_grid = gpd.GeoDataFrame(\n",
    "    df_base,\n",
    "    geometry=gpd.points_from_xy(df_base[\"lon\"], df_base[\"lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(\"EPSG:5070\")\n",
    "gdf_grid[\"geometry\"] = gdf_grid.geometry.buffer(500, cap_style=3)\n",
    "gdf_grid = gdf_grid.to_crs(\"EPSG:4326\")\n",
    "del df_base\n",
    "\n",
    "# Load tract shapefile\n",
    "gdf_tracts_proj = gpd.read_file(tract_shapefile).to_crs(\"EPSG:5070\")\n",
    "gdf_tracts_proj = gdf_tracts_proj[[tract_id_col, 'geometry']]\n",
    "\n",
    "# Overlay cache\n",
    "overlay_cache_path = os.path.join(output_base, f\"grid_to_tract_overlay_{decade}.parquet\")\n",
    "if not os.path.exists(overlay_cache_path):\n",
    "    gdf_grid_proj = gdf_grid.to_crs(\"EPSG:5070\")\n",
    "    overlay = gpd.overlay(gdf_grid_proj, gdf_tracts_proj, how=\"intersection\")\n",
    "    overlay[\"area_overlap_m2\"] = overlay.geometry.area.astype(np.float32)\n",
    "    overlay[\"combo_latlon_id\"] = overlay[\"combo_latlon_id\"]\n",
    "    overlay = overlay[[tract_id_col, \"combo_latlon_id\", \"area_overlap_m2\"]].copy()\n",
    "    overlay.to_parquet(overlay_cache_path, index=False, compression=\"zstd\")\n",
    "    del gdf_grid_proj\n",
    "else:\n",
    "    overlay = pd.read_parquet(overlay_cache_path)\n",
    "\n",
    "# File processing function\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        df_weather = pd.read_parquet(file_path, columns=['lat', 'lon'] + group_cols)\n",
    "        df_weather[\"combo_latlon_id\"] = df_weather[\"lat\"].astype(str) + \"_\" + df_weather[\"lon\"].astype(str)\n",
    "        day_val = pd.to_datetime(pd.read_parquet(file_path, columns=['day'])['day'].iloc[0])\n",
    "\n",
    "        merged = overlay.merge(df_weather, on=\"combo_latlon_id\", how=\"inner\")\n",
    "        if merged.empty:\n",
    "            return f\"No match on {day_val.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "        def weighted_avg(group, col):\n",
    "            return np.average(group[col], weights=group['area_overlap_m2']) if not group.empty else np.nan\n",
    "\n",
    "        agg_dict = {var: lambda g, var=var: weighted_avg(g, var) for var in group_cols}\n",
    "        agg_dict.update({\n",
    "            'area_covered_m2': lambda g: g['area_overlap_m2'].sum(),\n",
    "            'n_cells': lambda g: len(g)\n",
    "        })\n",
    "\n",
    "        summary = (\n",
    "            merged.groupby(tract_id_col, group_keys=False)\n",
    "            .apply(lambda g: pd.Series({k: f(g) for k, f in agg_dict.items()}))\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        summary = summary.merge(gdf_tracts_proj, on=tract_id_col, how='left')\n",
    "        summary = gpd.GeoDataFrame(summary, crs=\"EPSG:5070\").to_crs(\"EPSG:4326\")\n",
    "        summary[\"day\"] = day_val\n",
    "        summary[\"geometry\"] = summary[\"geometry\"].buffer(0)\n",
    "\n",
    "        year_str = day_val.strftime('%Y')\n",
    "        out_folder = os.path.join(output_base, f\"year={year_str}\")\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        out_path = os.path.join(out_folder, f\"{day_val.strftime('%Y-%m-%d')}_weighted.parquet\")\n",
    "        summary.to_parquet(out_path, index=False, compression=\"zstd\")\n",
    "\n",
    "        return f\"{day_val.strftime('%Y-%m-%d')} processed\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed: {file_path} → {e}\"\n",
    "\n",
    "# Collect and batch files\n",
    "all_files = []\n",
    "for year in years_to_process:\n",
    "    folder = os.path.join(input_base, f\"year={year}\")\n",
    "    if not os.path.exists(folder):\n",
    "        continue\n",
    "    all_files += [\n",
    "        os.path.join(folder, f) for f in os.listdir(folder)\n",
    "        if f.endswith(\".parquet\") and not f.startswith(\".\")\n",
    "    ]\n",
    "\n",
    "batches = [all_files[i:i + batch_size] for i in range(0, len(all_files), batch_size)]\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    print(f\"\\nBatch {i+1}/{len(batches)} — {len(batch)} files\")\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_file)(fp) for fp in tqdm(batch, desc=f\"Batch {i+1}\")\n",
    "    )\n",
    "    completed = sum(\"processed\" in r for r in results)\n",
    "    print(f\"Batch {i+1} done. {completed}/{len(batch)} succeeded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo_env)",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
