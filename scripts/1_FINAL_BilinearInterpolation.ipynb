{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1e015-6910-42ec-8f2a-bf5a02b192fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Set base directories\n",
    "era5_base = r\"# path to ERA5 daily extracted parquet files\"\n",
    "daymet_base = r\"# path to Daymet processed daily parquet files\"\n",
    "output_base = r\"# path to save final output with interpolated values\"\n",
    "\n",
    "# Variables to interpolate\n",
    "vars_to_interpolate = [\n",
    "    'wind_speed_mean', 'wind_speed_max', 'wind_speed_min',\n",
    "    'sp_mean', \"sp_min\", \"sp_max\"\n",
    "]\n",
    "\n",
    "def interpolate_file(daymet_year_dir, output_year_dir, fname, era5_data_cache, interpolator_cache):\n",
    "    date_str = fname.replace(\".parquet\", \"\")\n",
    "    daymet_path = os.path.join(daymet_year_dir, fname)\n",
    "    out_path = os.path.join(output_year_dir, f\"{date_str}.parquet\")\n",
    "\n",
    "    try:\n",
    "        df_daymet = pd.read_parquet(daymet_path, engine=\"pyarrow\")\n",
    "        df_era5 = era5_data_cache.get(fname)\n",
    "        if df_era5 is None:\n",
    "            return f\"ERA5 file missing: {fname}\"\n",
    "\n",
    "        df_era5_clean = df_era5.dropna(subset=vars_to_interpolate)\n",
    "        if df_era5_clean.empty:\n",
    "            return f\"ERA5 file empty after dropping NaNs: {fname}\"\n",
    "\n",
    "        era5_lats = np.sort(df_era5_clean['lat'].unique())\n",
    "        era5_lons = np.sort(df_era5_clean['lon'].unique())\n",
    "        era5_coords = df_era5_clean[['lat', 'lon']].values\n",
    "        daymet_coords = df_daymet[['lat', 'lon']].values\n",
    "\n",
    "        tree = cKDTree(era5_coords)\n",
    "        distances, _ = tree.query(daymet_coords, k=1)\n",
    "        df_daymet['interp_dist_km'] = distances * 111\n",
    "        df_daymet_valid = df_daymet[df_daymet['interp_dist_km'] <= 9].copy()\n",
    "\n",
    "        if fname not in interpolator_cache:\n",
    "            interpolator_cache[fname] = {}\n",
    "            for var in vars_to_interpolate:\n",
    "                pivoted = df_era5_clean.pivot(index='lat', columns='lon', values=var)\n",
    "                grid = pivoted.loc[era5_lats, era5_lons].values\n",
    "                interpolator = RegularGridInterpolator(\n",
    "                    (era5_lats, era5_lons), grid,\n",
    "                    bounds_error=False, fill_value=np.nan\n",
    "                )\n",
    "                interpolator_cache[fname][var] = interpolator\n",
    "\n",
    "        for var in vars_to_interpolate:\n",
    "            interpolator = interpolator_cache[fname][var]\n",
    "            df_daymet_valid[f'era5_{var}'] = interpolator(df_daymet_valid[['lat', 'lon']].values)\n",
    "\n",
    "        df_daymet_valid.to_parquet(out_path, index=False, engine=\"pyarrow\")\n",
    "        return f\"{fname} complete\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed on {date_str}: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in range(1980, 2020):  # Update year range as needed\n",
    "        daymet_year_dir = os.path.join(daymet_base, f\"year={year}\")\n",
    "        era5_year_dir = os.path.join(era5_base, str(year))\n",
    "        output_year_dir = os.path.join(output_base, f\"year={year}\")\n",
    "        os.makedirs(output_year_dir, exist_ok=True)\n",
    "\n",
    "        all_daymet_files = sorted([\n",
    "            f for f in os.listdir(daymet_year_dir)\n",
    "            if f.endswith(\".parquet\") and\n",
    "               datetime.strptime(f.replace(\".parquet\", \"\"), \"%Y-%m-%d\").month in [5, 6, 7, 8, 9]\n",
    "        ])\n",
    "\n",
    "        print(f\"Processing year {year} â€” {len(all_daymet_files)} days found\")\n",
    "\n",
    "        era5_data_cache = {}\n",
    "        for fname in tqdm(all_daymet_files, desc=\"Preloading ERA5\"):\n",
    "            era5_path = os.path.join(era5_year_dir, fname)\n",
    "            if os.path.exists(era5_path):\n",
    "                try:\n",
    "                    era5_data_cache[fname] = pd.read_parquet(era5_path, engine=\"pyarrow\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not load {fname}: {e}\")\n",
    "            else:\n",
    "                print(f\"Missing ERA5 file: {era5_path}\")\n",
    "\n",
    "        interpolator_cache = {}\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    interpolate_file,\n",
    "                    daymet_year_dir,\n",
    "                    output_year_dir,\n",
    "                    fname,\n",
    "                    era5_data_cache,\n",
    "                    interpolator_cache\n",
    "                ): fname for fname in all_daymet_files\n",
    "            }\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Interpolating {year}\"):\n",
    "                result = future.result()\n",
    "                if result.startswith(\"Failed\") or result.startswith(\"ERA5\"):\n",
    "                    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo_env)",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
